version: "3.8"

services:
  # --- 1. Edge Router / Reverse Proxy (Caddy) ---
  caddy:
    image: localhost:5000/caddy:latest 
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - caddy_data:/data
      - caddy_config:/config
    configs:
      - source: caddy_file
        target: /etc/caddy/Caddyfile
    networks:
      - web_proxy
      - monitor_monitor-net
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  # --- 2. Main Application Server (The Secure Bridge) ---
  server:
    image: localhost:5000/aci-app:latest
    command: /bin/sh start.sh
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/v1/health || exit 1"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - services_network
      - web_proxy
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  huey_worker:
    image: localhost:5000/aci-app:latest
    command: huey_consumer.py aci.server.tasks.tasks.huey -w 20 -k thread
    networks:
      - executor_network
      - services_network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  # --- 3. LiveKit Server ---
  livekit:
    image: livekit/livekit-server:latest
    command: --config /etc/livekit.yaml
    ports:
      - "7881:7881/tcp"
      - "3478:3478/udp"
      - "20000-21000:20000-21000/udp"
    configs:
      - source: livekit_yaml
        target: /etc/livekit.yaml
    networks:
      - web_proxy
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  # --- 4. Voice Agent ---
  voice_agent:
    image: localhost:5000/aci-app:latest
    command: python -m aci.voice_agent.agent start
    networks:
      - services_network
      - web_proxy
      - executor_network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  # --- 5. Internal Services ---
  gotenberg:
    image: gotenberg/gotenberg:8
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
      interval: 15s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - services_network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  code-executor:
    image: localhost:5000/code-executor:latest
    dns: [ "8.8.8.8" ]
    networks:
      - executor_network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  searxng:
    image: searxng/searxng:latest
    configs:
      - source: searxng_settings
        target: /etc/searxng/settings.yml
    dns: [ "8.8.8.8" ]
    networks:
      - services_network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  cycletls-server:
    image: localhost:5000/cycletls-server:latest
    dns: [ "8.8.8.8" ]
    networks:
      - services_network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  steel-browser-api:
    image: ghcr.io/steel-dev/steel-browser-api:latest
    environment:
      - DOMAIN=${DOMAIN:-localhost:3000}
      - CDP_DOMAIN=${CDP_DOMAIN:-localhost:9223}
    networks:
      - services_network
    deploy:
      replicas: 0
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '2.5'      # Still recommended to handle CPU spikes for one heavy browser
          memory: 3.5G       # Reduced based on your observation (max 1GiB usage)
        reservations:
          cpus: '1.5'      # Guarantees 1 full core for the task
          memory: 2G       # Guarantees the observed 1GiB of memory

  headless-browser:
    image: localhost:5000/headless-browser:latest
    networks:
      - services_network # Connect to the internal network for the pool manager
    environment:
      - LOCAL_PROXY_URL=http://local-proxy:3000
    deploy:
      replicas: 2 # Start with 2 replicas, you can scale this number
      restart_policy:
        condition: on-failure

  local-proxy:
    image: localhost:5000/local-proxy:latest
    networks:
      - services_network # Connect to the internal network for the pool manager
    env_file:
      - .env
    environment:
      - PROXY_BIND_ADDRESS=0.0.0.0
      - PROXY_PORT=3000
      - PROXY_INTERNAL_BYPASS=headless-browser
    deploy:
      replicas: 1 # Start with 2 replicas, you can scale this number
      restart_policy:
        condition: on-failure

  skyvern:
    image: public.ecr.aws/skyvern/skyvern:latest
    env_file:
      - .env.skyvern
    volumes:
      - ./artifacts:/data/artifacts
      - ./videos:/data/videos
      - ./har:/data/har
      - ./log:/data/log
      - ./.streamlit:/app/.streamlit
    networks:
      - services_network  # Connect to your existing network
    healthcheck:
      test: ["CMD", "test", "-f", "/app/.streamlit/secrets.toml"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 0
      restart_policy:
        condition: on-failure

  skyvern-ui:
    image: public.ecr.aws/skyvern/skyvern-ui:latest
    env_file:
      - .env.skyvern
    volumes:
      - ./artifacts:/data/artifacts
      - ./videos:/data/videos
      - ./har:/data/har
      - ./.streamlit:/app/.streamlit
    networks:
      - services_network  # Connect to your existing network
    depends_on:
      - skyvern
    deploy:
      replicas: 0
      restart_policy:
        condition: on-failure

  postgres:
    image: postgres:14-alpine
    restart: always
    volumes:
      # This saves your database data on the host machine
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=superdupersecretpassword
    networks:
      - services_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 0

  redis:
    image: redis:7-alpine
    networks:
      - services_network
    volumes:
      - redis-data:/data
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

# --- Swarm will create these as overlay networks, allowing services to connect ---
networks:
  web_proxy:
  executor_network:
  services_network:
  monitor_monitor-net:
    external: true 
  
# --- Swarm will manage these volumes on the host node ---
volumes:
  caddy_data:
  caddy_config:
  redis-data:
  postgres_data:

# --- Define the configs that were created with 'docker config create' ---
configs:
  caddy_file:
    external: true
  livekit_yaml:
    external: true
  searxng_settings:
    external: true